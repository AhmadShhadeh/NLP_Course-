{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn0AAABmCAYAAAC+/Se8AAAeqUlEQVR4Ae2dL4wlOZLGBx5cuHDhwYMLDw7pqpZOK426qqSCDQ8OHDZw4MCFLXVX1aDTwoELGy5suLDgwD59fhmv4/nFF2lnvsz372up5Uw/px3+ORwO/8ms777TPxEQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQARG4SgJ/+7/HP98+3f39zdPd7/h/8/LwnoFA2ref7v87+s+emRL/9sMPf6nL6M3n7W+Pf/qfp8f/7H3uUtOfIo9TlOlS21/1igmcov2LJT2P2B6e51EjSTmLQD2QT71/83z311mC6OFC4O3Hd/9183T3evN099X/v32+/0eE6M3Hd48+nb+O0k+NK87nrkyfx/KCA3H7dPfj8OxOnW6f7/99+3T3K+o7lk/2O5zIUGdn5gvZw3zhYP/2+KdMJvbbGjxY2Sz+FGVisir+tAjA5tM+Qiaie+mrvrSW/bt9vv9gk+qd8OO7x9OiPE+aXp7zStPTZ0HAOwkzr7+cRYVPXMibp7vPrB3efLr731r8NZw+rPLVMkWyeNnePN2/hWNXPxfdw6HFbNQ/33p9+3z/T5LnH4+/P/5Haz51OjirUb6IQ93q9GP3a/EYk8P/fooyefl0fdoEbp7uvrA+0hoPJ9DXci37R2X/dP+Tl+fcr3t5nnt9JX8DgdbO2ZBOTl8D7ywJHJ+MMxyc+vk1nL7IAcqcNMyis3pEv8FBrAeAuq7RfbQqus3/5eGH6JmWOOZMIm/waMnD0qzJw8ocC09RpjGZ9ftpEaCO0+6OwM6OxbZvDml8n1/T/lHZL8jpm8LztDRM0ixCoO6EM+7l9M1soWEpPjOSe4xXcfqeH/7l9YJtNaP6WAH0aXuub5/v/7h5uf++FWODUfvQmpdPF61s+nrAYfLps+s1eWRy+N9OUSYvn67PgwB1nCY6fWvaPyr7BTl9U3ieh+ZJylkE/GA283rPIZkl2BU+jO3IrA1un979VmNZ2unDuZ09mcgKGs7XFcetw+jXefc4fnAQ6+f9PfKassUbrWz6fLFlUrdDdL82j0iGOu4UZapl1P15EKCOU0f/9yt9a9o/KvsFOX1TeJ6H5knKWQSqwSxbZRr7TU7frJbYPHz78vAza5PoZZmlnT68bFHJ88ocKTilVdoxnWG/f2ZleMQNztnXG+Kg+nzq62xrF/VrdSbX5lHXI7o/RZkiORV3+gSo4zTR6UON17J/VPYLcvqm8Dx9rZOEswkcaJDG4C2nb3ZrfPcdnB18rqVql1c4d1H2Szp9w0xx963bp7tfIzkQx17cgJOEOmFWj5WmQebwEDgcrta3Y1vOpfVsxaIOY1u71i7YOmEcLH5tHlZuFp6iTJm8+u10CVDHaYbTt5b9o7JfmNPXy/N0tU2SHYyADWJ74YUp/8GArZQRnIriHL3cf5+9NLGk04dVslovotVGIMnO10HGGltkjOCgIb5Oy+6zN9NM7tZVOSujafUQg9rICuIxeFgdWHiKMjFZFX/6BJjjhAneXOmXtn9M9psLHfdaec5tNz1/BgRscNwLL1T5z6BJukRc0unDCxteL26fH/7FhMMqnk/rr7NVMVutw7YOy5vFw6Hz5dDrEQfN5z+2tWtljMl7DB6+HtH1KcoUyam48yDAHKdDOH2tBKbaPyb7pTp9rTyV7goI2CC2Fx7I6Sud8tP9T+hM/n/9rTM4BlhlGT7kW7b+sBWFe8SPbfnhd/zlipL+29umr8Pzv2JbcW5zogzUB45K5Rx8LmelXh7e16ty5U3Jqu7ggAE4kgccPCd/HaWfavSivHxctCqEdvBp/DX47umQbfMkuoSVvbFVM1+OXaflWblD2LrF27q1i3qivU2WKEzlW4BHJEMdt5ZMrX0e8qB/YHKBCUXh+nz/x9Bn/w696Fn5retb36/Rf1Fma/3PwebVDP09c5zmOH1L2b/St70dDj6AD/2D7nmbi+d8ndl1yf/l4T3sQsnjmw3ajEH4GPTHd4/1+MDys/g9uYc6eLmQb+kzm0nwF/C3HZleniiX6m+1Y1Psyaf7nzbj4ab/lro/3/2CPA7Rd8s4BK6bBQh/JOhLiTvAmGusryacMlD3wKk6wPbgvhkGNKqt9lBZhsPz0Z8jg2Jlh399nuiQY85jVLfimGyUe3RlqZxfe3n42cphhhGGJSqrdLhvBmPLC/U4RPoojygu+qxHZrAKIyI3vqUH4xOVMzUu2nr2be2vW7d4m7d2iz7yVU/UaW0eLRzXkqmpz++/ILSj6679XqN+31JfS7Nm/0WZTfVv+JZlWckO/gQj6rO0zTN2Wchsm9n27Fn221L2L1vldrq2p4Nscm7yw/kqE35u+/byBB/vtFleUcjkRnxxiMjH/M3e9vKEDEx/EY/fW+uMiZw5n1HdsjiMn9Dxlt2ckgYLKcNfd2F6ycbcTI6L/I0qPHFKeiEwBSqKT/7kGJUJHev57heTATMNWyFIn9ntkJ9bOxzKQVqqRLv51p37y7Cy4mcn39IQvr2dtDe9sRsL6/NymFWNPTNi/F6nGoCo3NZBb6sXDVu81ertt7Yi7YzBN5LN4tbkYWWOhWvIlPb5zTGAnZeDtm1EOON35GlGfayO/ve1+y/KTut/BjbP88uumV28FqevrAgmOjuq12QM8MyZ04fPVbGxD/GWx5Txgekv4rFDl34QP+LRUE+TF+HEcf0r6j1lzPVlX8U1VczOhmKwmAKhgVq8+Eg+KDIGAGZ0omd8HGRi8vp4zJbY244+v+Qag1s8wBG+vZ20N72vH7suWwJ1521wmsLnXD6blYv2jy8z+RA/4rzsOWxjW7yDc7D3XNK2X202zeRckweToY5fQybW59Ffp/Z5tANsRteE7Qj9F7xZ/c/B5tX6kt0z+3vpTl9ZaW1Yqc1sh/0Gu5RNHpnTN2x3hvbKH8OZMj5w/W08Q+1svtUT9ch0yX4b7FM8Zgb5Wv4ufKU2hoy5VvbVhA5WrUBf0Pit/9mgyhQoKbeWY+9+aFT6N2pb8kZnyBp5jlPZUj5bau7tpL3pszrbb1hNrepAv81nz1jYslU/d7sOZWEArWQsesKcdOhMZlzZrJ3lV8pucITX4mH8W8KlZVqiz1tbYzU2a0er/7H6L8pfov5r2Dxj1xpSp284i90ydtQTp1571pqeOU+mVyyMnBU4tSz9lPjMSZ4itz+G08rHt/kS+tvyQfthuzreGWtz+PZ8hZ32kNO3aeYdKPPAht/p61QgeOn/nilTkzOIlSKv6PX1oTv2Xp2IAvZ20t70dT3r+2EWu9MGmVGqn28ebN02fZ3H2D1k3ONpurs59BufvUycNOpElnMlxMFseON4DR5jvOrfl5aps8/nhtra1Yek7/h6Hqv/QobO+p+MzfP8Wq6Z00f7pm/D4bp2qnrtWWv6Kc4T6lHLhwnrWP3Q/uWj9sNLF4M+5KtXwdlNtEGL3JgQDGWUhRrfdq18dp7ByyxBW82Ngyy+nPq6d/emW54Gu1HLdJH33eC4Mkx2+rDy4LdtMPvrMdoYsOu3gYc/0ZV1tFfWoGNbYGAG+dAhMYAiH4S4b5abKGBvJ+1Nz+ps8cOZjZ2BuDZ8lpaFaMsW5x1bFMaP5RXFZ+1TdKf61IzpOPQsym84B7JTZ3sG5xCZMRqbOFhZS/OwcnrCJWVqcXowUJW38t22T5npl7NKu58KsrawEM/61Yy63pl+bPNYqP9Clsb6n5TNqxm23J+V0zfY52KzYafJ4sLWrsO2O/sOHtC5YcU1tBXlWfK2b5loJS8DMp2GDKazJPzibai/Lrr48d0jeS58MbA80+b0ffZ/Jx1yBn+9aYcTzmEzvWqoJ/4S0j8w3tnZ8NLPXx5+aNq5gN9Cxlwm08XGM4WYED/N6UsaYkyJBhlfvcPoGwrKkdWDPccGeeRVjMVLfi5tc8h2ZMWS1LvXietN7/lE10EHCts1etbHDQNv5nQXgwCHnbWDz89fZ7NtrAIyJjCs0dYg29rFoIZy2UsjkN3LlV0vySMrN/ttKZnGnB5wg6OdyTa0IdefZKX4mP0XdRqrfzb4HMvmZW3Bfjsnp6+uA5Wd2GU8Hxx72To10Ne6jOh+GBvinYhAp8ecIXOAorIQx2whxjL6zIjTl01287OGfHdt5CU6+heprA4brvGOzNYHSNrW8rmKcAuEr+BtFXskbegcjBjA9A/Xj82sIE+9wlc3WqZMUYeBA5LVEwNlXUZ0P+ZwMsPf20l700eyWhxmiXsz2RkdJX2Talffuj7pwoyvOWFhPay8YIsXz0VtbjPT7PMwkRNpPOtwKR51OT33S8g00udfs1U6L3u06uzaKVypP3b/hfwj9T85m+eZ91xTx8n6WkMIp8aX2WvPetNbWVR2Yu/Qz/dso9UvcNasnChk9iSalGZOH8a2KH8fN4VPpr+wlfVqoi8P/Y9yeroLdR/5uX6952/UOuLL89cjZWulz2BlsDt/63f6ggHY5LIwc9qyGYc9H31vzuoVKVPWSfznYiz/LExn7cS4ZOVHZfWmj/KwuGgFDR3Jfp8SDoNwyznLZsePzSa9PtA01Rbv4PTsGRroiE0KhhWxME3rJMDYLcHD8p4aHlqmbNCAvvbIydoR7ROxz/rDGv0XdcvqjwF/rP5r27wxedjv1HEyZ6ghrG1w1n6RHL3pLQ8qO7PLm0+VRDageRJjZSOsP4nFxqTM6WMLB76cKXwy/fVvBvty/HWy0k58hPIZmIgtjlHRv/Xuy7Tr9FurpG3t2asJTdnqsDS8/4L5yDWcqwhapkDZjMHyYqs6kJeVac8iTFcLAgPMHLVoFubLia4xO6y5bu+JAvZ20t70kZwWVw82aDv7bU44rLzt/Em3LQc3MGDrvMXJpOdx3JkRxqVux7GtXdQ7bcdAh8ZYHZrHWHktvx9SpqTPh0Y/ky9bMY/6/7H7L+qS1P/rKdq8jH/2G3Wc8MHgkfHCfq/7O+u3sBeRLL3pLQ8qO7HLbByCLUJ79/5vLT91+hpszxQ+mf62rNIzVqiz8fdhYoO/9uykIM/BVsfHQkjbelmu4joafEvcgQAlChQu9dbQM4XwB0nr5+w+6zToEJbOwmRloUley8dCtnWIelkaH/Z20t70vix/Ha14RXz8M73XbECudPBz1tExaFbptzNEv9U/ODHx2RlnLFn72Nau1bE1naVvCQ/Bo6WcnjSHkIn1eb8S2ypT6nAH22rH7r+oF6t/y2cr8PzaNq+1Lep0zHHBCw112tb7XnvWm97kYLIzu5ysXm3tD7NLPfH1y2bZ+IXfrD4snMKH6S8cXFaOj0/0N3T62MuPsLk+39brevFiy5+Mua35Xky6LRC34lLiDgSIKRDiWyBGW44mc7S9U+cZOTP2fOTUMIWpO2NdDrunxoLw7e2kvempnC8PPxsXhPWKGHuuNz5rTys/WsGxcjIjiLa2dAiZA2BtmerG891fd/J6evebyefDKY6Mz3cuD5/Xoa7nysT6fO1It8pLHe7AuTh2/0WdWP1P1ea1tkOdjjlOF+n0Pd//0/f7pa5rHemxd3X7FF087Nu7TQsfyZgUO33sawsjn1WL6os4OnElYy7L52LjqfIeCNBcA5go0Nd6ayBqpOG8UjgbQ971M9SQua3D+pnsns1i2Iwyq29UTm/6KA/E1Vumcww3K8Pi0zfYhjek2WofHMJIZ+GkWv4WMjbm0PbMSA/xBq/JVYdzeNR5Hep+jkysz8OZnCIfyy/S0WP3X9SPyYv4lvozvYXeL2HzWmSK0lDWgTMePR/FZXU/RHrLg8nO7DJNXy+WzLyvV7cyp69FF3p5gs+C+hs6fay8qH9b+2Uh3V4+kE+TlX0Wv0UDaIk7ECDaoA1/yxUAM6VtOR/T6/TRlYKpsw4yi2HGJatvpFC96aM8MMDXetCydRDl1Ro35lTg9ygvOosL3gwb2+KlK0iBg8/euAM35qBG8rO4qTxYfoeInyoT6/MwxlPkqickpqvQhTq/Y/dfyMPqj5XnWt7oPuvTS9i8SIaWOOYITR2sUWZW90im3vSWB5Od2WWmV6aLBwx3HKNrcPqgL4Rf08qitamFbIeHta09dzUhgX2w15upAWycDU7t1NaA3U4f28abfr5g569bbHkTp7q3vr3pjYsPg063Y3h82kNes1U7MGJbgcz42pZtLR81ADhsTmbl9tauz+uQb/D6fP31FB7++SWup8hE+3yj0+PrASeHtVNkxNlxinoFxZeRXTOHMyrb8qH1P1GbZ3L3hsxxukinj4wLmMjAMTvU/9r2XInT92vUx6PdmxYdndJnW/K9mDQR7BJHnJLeip+bAWTbeGDS8uaS55M5nGzQ6HXietN7+XCNVSp0rh09mNn25axcw6rOcEg/fNOKOXE3T3dx+qe7H+u64T7js1Pnbw5g6PAOsjYfE/CyLMnDl9NzvaRMrM+j7XpXRbO379G2dZ2P3X8hD6t/qzOU6Wxd3+g+szsRsyiPlrhrcvrYlmHr6m0LzyjNNTh9mb736it2J4hdP9hCVtROZxW3NKBzM4BZJ+s9tJ+sMlEFzDpApFi96es8oucxaNTpWu7Ldqp/IaThDBfXj/2vt6eD2dP920jGdIv3m6O3debYCiPyZtvBbNtyaR5Rfcfi1pCJtSlsTca3ln0zIYk/nI28Ij09dv9FHVj95fTVLbx/H9kjG6P2U+eTuii9xTGHlU3GmTOBCXPvYoDJ0BJm+hzpf51nL088z/QX8XX+0X1SZjihBj9r4yBs/g7iMDH/EuSxse8zFzOiup5l3NKAmAKdqgHcDDTVypd3DtznPrIGz86AFeZEAZMOc9DvVJnsdftg+9R+6wmxIlMvq8MgZm9YD500XLmLjG+26pMZwNT59m1LPvhrHPjW4f5ZrTV4mFyt4Voy1TrlbcyYTvi6QAf8s9V1OIAcu/9Cflb/U7V5nnnPNXOcWusZlbWW/WOyR3YHcmZ6BZuJ36P6RHGwVZh8Rb/Vcdfg9KHO7NgO+jx+G3OsS/vws4Fy+rxiVYb024qH/UHyxvMK9VkEK+McDSBbyh8UsPyheKtfFOKr4HtbppVzwYzLWkYPcsP47LV/w+pcXWcY+b18vtX3lelG9hwclKCcH0k54Z/ksudHHfBB1rFzX+xr73B2rSyEWb2wxXkoHr7Mses1ZWJ93rVd+rc0BwMenvOxPHDWkNX5mP0XMrH6ow2YzD6+1wb4Z3Ed9utBx5F3nX7qPXOcWusZldtb9970ViaTndllPDeiVx9aHDk3Nrxi1XvMmbkWpy+b0A99ntoM2FO6C/NtHKK7a6YTVxOaET1AGM68z9EAoiOOOW2oFwYedEqkRzh06LbvOZ3ASl+9klJWYRpnoL6DtDhVOKOHdOA0GGr6IgV0MVq5K3n4Trx11vLVyWyWvqP3pE2srplh8gZ/LR4mV0u4pkysz++w3szgoRPvoRPgZ32I/Ykqex5Odraycsz+i7Zg9W91hqY6MqYHcvruyuKF8YjCKU5fg169YkzAeVlfZnkJ7OXhh2g1CzYXehHZO+SBPmF6X4fsGV/2FF1i+ot4nze7TsoMfQTLJ+JT13m4/wxZmJzkGTl9BpoCCgbXkbRhg7KGOXUDmCjudjV0hEeejjgYWbnWZj7sTe+frQ0fe3nCP8OumUM2hREzLswZiD7dUcvZIl9trOs8yssPpF/Uq3ct5bWyYTxq+cbu15KJ9fnW+o6lw0A4VtesX4zl3/Q76b+Qi9X/1G3eGNP699p+GLfWetb5FXadHxPO2jnK3+KY7NlKH55tmTwZB2av3O/bMYLtMlyT0zc41fHXLojdjVjSuKTPml5cRUgB9UO+KKcPjQ/jdUA+2w5e8iQK2GvEetObUofGhHwbz57JwgN2WHpwl62+Zlt9JvOYsWZG1563kMlQf3h4DR4mU2u4lkzM6TlIXyL9JmJwjP4LOVj9W52hqX3aGGilb5mVPuObvSE+Rcexcs1W7UI7PYzN7BmTs+hipxNdnnm6+z2qB/Ta582uE/0NfQSfz/BprPicd79P0jTm+vKv4jpq3IlxYYOeswEctgU/TOQBhcObRLECk8Er6TAHfZGj/shxfS5tivKjwyKfGby+wjmLyh6MwW4nNiPQ4KyObvGS9qhlobP34BM1S/Ko5Wq9X0Mm2uefy5u4cX+wtszCgHFW72P0X8hD668zfVlzld/Wsn9TV/qsAvXRmBk27xV90vKtw2tz+lD/YUeFv4U7YiPmtm3dBhd3P0NZ6wH44pw+a+zsAC/jh/MJOKfUq4BrGD0MhrUz2vMpDeMShcMqw5QOW87DRHkiLlupwwoWe87HZ9ubY1u7lg/Lg32vaykeJs+UcGmZMqdncN679GM49xR+h7Gl/mv2X8iT1b9F3l4bUOc5tG9tn8s98q7TT71ntq11RTMqt7fuvemtTCb72PauPY9wsEmTJzGwGWO26xqdPrAtY+fz3S9sfK3jsdhgus0WHlp2hHz7Xux1DW/G/cU6fUUJN6tYoy9pFIVzb8D2GpdeI9abHnWJHKhWp6elI8CQDQNtk0GEIzVm/JItlfTNXS9vVG/oe+vWLvJqfYPXl7sED5//lOslZRpzeopB33yOZVQ/8JkcODFT6uifGVY4F++/KHOs/l6u6HpKn/b5yOlbdnt3hzUm9o26bGNr0Q+yo+HzxvW1On3GAbqcvSCJxZXyQuXwAuKwoBFOeGD/LV+FItBMoBjUl4f3mM2i89p/bJdGnxlpznjFhPV369Bxligegzs6JMoDp7Ji83z/R7nGeclP9z9lWxtLyHTMPE+RxxIyoX1tgPNhvQJUDPTmrcYP1o8G3fgVjs/YRGBKW15C/51Sbz2zPIHy6RB8rqseG/D31+EYvjy8X0Knl6/Z+ZSQOsnJNvr51FCSikAngbLCU5+JcCuTndkpuQjsEWh1+vYeVIQIiIAIzCBQn1X3k05MMmdkrUdF4DwJ1NuTWH3Das951kZSnyIBOX2n2CqSSQROk0B25ha7UK1Hj7CS6p08f73UbtZpEpVUIuAI1F8ux3k697MuRWA2ATl9sxEqAxG4GgLDy13xObzNR9zLX8HC1m0NBat3OFZVH1nyDh+ucVykflb3InDxBHDmZK8zBH/u7OJBqIKLEpDTtyheZS4CF0eAfRWhHq+G+1dmY0j6L9ravTiVUYVaCNTL6HjTWJ2hhZzS9BBgBrl+kaMnT6UVARG4XAI4YkS/dFGfQe+8j1YIL5ekaiYCjgCW0dEBtv/1NpOjo8tDEZDTdyiSykcErocAzu6xb+yRFTy6JWzpta17PfqjmoqACByJgJy+I4FXsSJw5gTwdQm8dGFO29QQLyjK4TtzZZD4IiAC50FATt95tJOkFIFTJQCHbep2L46R6HuIp9qykksERODiCBSDjY/RVv/P5ePlF9cgqpAInCmBm5f772FH2ERyWAl8LS+CvDy8x8fXz7SqElsEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEDkjg/wHV/IhjSoM8tQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email/SMS Spam Filtering![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JA7d8h-cQi2"
   },
   "source": [
    "# 1- Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Background "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam filtering is a beginner’s example of document classification task which involves classifying an email as spam or non-spam (a.k.a. ham) mail.\n",
    "Spam box in your Gmail account is the best example of this.\n",
    "So lets get started in building a spam filter on a publicly available mail corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,IFrame,Image, HTML\n",
    "from IPython.display import Image\n",
    "#from IPython.core.display import HTML \n",
    "Image(filename = \"Spam_image.png\", width = 600, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 About The data sets which will be used "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8357GZ7cbpq"
   },
   "source": [
    "SMS Spam Collection Data Set\u000b",
    "Download: Data Folder, Data Set Description\n",
    "Abstract: The SMS Spam Collection is a public set of SMS labeled messages that have been collected for mobile phone spam research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#from IPython.core.display import HTML \n",
    "Image(filename = \"usi_image.png\", width = 600, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Understanding the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ckyswi1Vl7d-"
   },
   "source": [
    "**Importing The required python packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXP5brtZlpbt"
   },
   "outputs": [],
   "source": [
    "\n",
    "##Load the necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Reading & Loading data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ham  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup fina...                                                                                                                                                    \n",
       "1  Nah I don't think he goes to usf, he lives aro...                                                                                                                                                    \n",
       "2  Even my brother is not like to speak with me. ...                                                                                                                                                    \n",
       "3                I HAVE A DATE ON SUNDAY WITH WILL!!                                                                                                                                                    \n",
       "4  As per your request 'Melle Melle (Oru Minnamin...                                                                                                                                                    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "# the data sets are in the same directory where the code present, so no need to specify the path \n",
    "\n",
    "SMS_SH_df = pd.read_csv(\"SMSSpamCollection.csv\",sep='\\t')\n",
    "SMS_SH_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it seems that the file has no headers , let us create one \n",
    "#Header=Non: because the file has no header if we didn't say NON it will take the first column as a header\n",
    "\n",
    "SMS_SH_df = pd.read_csv(\"SMSSpamCollection.csv\", sep='\\t', header=None)\n",
    "SMS_SH_df.columns = ['label', 'body_text']\n",
    "SMS_SH_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Exploring the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5568, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame.shape ==>Return a tuple representing the dimensionality of the DataFrame.\n",
    "SMS_SH_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sIXXOUAynkOp",
    "outputId": "6a49b483-aef9-461c-df7f-eb18703c1341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5568 entries, 0 to 5567\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      5568 non-null   object\n",
      " 1   body_text  5568 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#dataframe.info() function is used to get a concise summary of the dataframe. It comes really handy when doing exploratory analysis of the data\n",
    "SMS_SH_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "6pLZjG4qn2ec",
    "outputId": "945fa957-f6c1-412e-842b-1049a55c0924"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5568</td>\n",
       "      <td>5568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4822</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label               body_text\n",
       "count   5568                    5568\n",
       "unique     2                    5165\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4822                      30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#DataFrame.describe==>Generate descriptive statistics.\n",
    "#Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
    "#Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail.\n",
    "SMS_SH_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4822\n",
       "spam     746\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many rows are spam or ham \n",
    "SMS_SH_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86602\n",
       "spam    0.13398\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMS_SH_df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SMS_SH_df.label.value_counts(normalize=True).plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change pie chart colors here\n",
    "colors = [\"magenta\", \"green\", \"purple\", \"orange\"]\n",
    "SMS_SH_df.label.value_counts().plot.pie( autopct='%1.1f%%', shadow=True,startangle=60, counterclock=False,colors=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  body_text\n",
       "False  False        5568\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many NULLs/ missing values\n",
    "SMS_SH_df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=SMS_SH_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation-Pre-processing text data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimate goals from the Pre-processing text data is \n",
    "To bring the text into a form that is predictable and analyzable for our task\n",
    "by\n",
    "Reducing the corpus of words that exposed to the model\n",
    "Explicitly correlate the word with similar meaning \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cleaning up the text data is necessary to highlight attributes that we are going to want our model to pick up on. Cleaning (or pre-processing) the data typically consists of  number of steps:\n",
    "Removing  punctuation\n",
    "Converting text to lowercase\n",
    "Tokenization\n",
    "Removing stop-words\n",
    "Lemmatization /Stemming \n",
    "Vectorization\n",
    "Feature Engineering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I love NLP will use python in our code\n"
     ]
    }
   ],
   "source": [
    "# define punctuation\n",
    "punct= string.punctuation\n",
    "text='Hello!!!, I love NLP, will use python in our code.'\n",
    "\n",
    "no_punct = \"\"\n",
    "for char in text:\n",
    "   if char not in punct:\n",
    "       no_punct = no_punct + char\n",
    "\n",
    "# display the unpunctuated string\n",
    "print(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                    body_text_nopunc  \n",
       "0  Ive been searching for the right words to than...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "2  Nah I dont think he goes to usf he lives aroun...  \n",
       "3  Even my brother is not like to speak with me T...  \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list comprhansion \n",
    "#lambda function\n",
    "#adding join to join chars into words \n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['body_text_nopunc'] = df['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda  function :\n",
    "A lambda function is a small anonymous function.\n",
    "A lambda function can take any number of arguments, but can only have one expression.\n",
    "The power of lambda is better shown when you use them as an anonymous function inside another function.\n",
    "List Comprehension\n",
    "List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.\n",
    "\n",
    "\n",
    "\n",
    "https://www.w3schools.com/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Converting text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python is case sensetive , is lower to save resources in compresion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'nlp'=='NLP'.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lower to teh remove_punc function\n",
    "\n",
    "#list comprhansion \n",
    "#lambda function\n",
    "#adding join to join chars into words \n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['body_text_nopunc'] = df['body_text'].apply(lambda x: remove_punct(x.lower()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</td>\n",
       "      <td>ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even my brother is not like to speak with me they treat me like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                                                              body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.   \n",
       "1                                           Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "2                                                                                                                                         Nah I don't think he goes to usf, he lives around here though   \n",
       "3                                                                                                                         Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                                                                                                                   I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                                                                                                                   body_text_nopunc  \n",
       "0  ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times  \n",
       "1                                             free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s  \n",
       "2                                                                                                                                       nah i dont think he goes to usf he lives around here though  \n",
       "3                                                                                                                       even my brother is not like to speak with me they treat me like aids patent  \n",
       "4                                                                                                                                                                 i have a date on sunday with will  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tokenization\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Tokenization is one of the most common tasks when it comes to working with text data\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    "\n",
    "Methods to Perform Tokenization in Python :\n",
    "\n",
    "    Tokenization using Python’s split() function\n",
    "    Tokenization using Regular Expressions (RegEx)\n",
    "    Tokenization using NLTK \n",
    "    Tokenization using the other libraries like spaCy and Gensim library\n",
    "    \n",
    "\n",
    "This is important because the meaning of the text could easily be interpreted by analyzing the words present in the text.\n",
    "The final Goal of Tokenization is : Creating Vocabulary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'NLP', 'will', 'use', 'python', 'in', 'our', 'code', '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#\\W+ regex, indicates that it will split wherever it sees one or more non-word characters.\n",
    "#So that'll split on white spaces, special characters, anything like that.\n",
    "\n",
    "text='I love NLP,will use python in our code.'\n",
    "tokens = re.split('\\W+', text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunc</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</td>\n",
       "      <td>ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even my brother is not like to speak with me they treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                                                              body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.   \n",
       "1                                           Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "2                                                                                                                                         Nah I don't think he goes to usf, he lives around here though   \n",
       "3                                                                                                                         Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                                                                                                                   I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                                                                                                                   body_text_nopunc  \\\n",
       "0  ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times   \n",
       "1                                             free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "2                                                                                                                                       nah i dont think he goes to usf he lives around here though   \n",
       "3                                                                                                                       even my brother is not like to speak with me they treat me like aids patent   \n",
       "4                                                                                                                                                                 i have a date on sunday with will   \n",
       "\n",
       "                                                                                                                                                                                       body_text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...  \n",
       "1                       [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "2                                                                                                                                [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3                                                                                                             [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                                                                                                                               [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "df['body_text_tokenized'] = df['body_text_nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Remove stopwords\n",
    "\n",
    "**Stopwords** are common words that are present in the text but generally do not contribute to the meaning of a sentence. They hold almost no importance for the purposes of information retrieval and natural language processing. They can safely be ignored without sacrificing the meaning of the sentence. For example – ‘the’ and ‘a’.\n",
    "\n",
    "\n",
    "**Stop Words:** A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "The NLTK package has a separate package of stop words that can be downloaded. NLTK has stop words more than 16 languages which can be downloaded and used. Once it is downloaded, it can be passed as an argument indicating it to ignore these words.\n",
    "\n",
    "    import nltk  from nltk.corpus \n",
    "    import stopwords  set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_En = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunc</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</td>\n",
       "      <td>ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even my brother is not like to speak with me they treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                                                              body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.   \n",
       "1                                           Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "2                                                                                                                                         Nah I don't think he goes to usf, he lives around here though   \n",
       "3                                                                                                                         Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                                                                                                                   I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                                                                                                                   body_text_nopunc  \\\n",
       "0  ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times   \n",
       "1                                             free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "2                                                                                                                                       nah i dont think he goes to usf he lives around here though   \n",
       "3                                                                                                                       even my brother is not like to speak with me they treat me like aids patent   \n",
       "4                                                                                                                                                                 i have a date on sunday with will   \n",
       "\n",
       "                                                                                                                                                                                       body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...   \n",
       "1                       [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "2                                                                                                                                [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3                                                                                                             [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                                                                                                                               [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                                                                                  body_text_nostop  \n",
       "0                                 [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "2                                                                                                             [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                                                                                                          [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                                                                                                                   [date, sunday]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_En]\n",
    "    return text\n",
    "\n",
    "df['body_text_nostop'] = df['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stopwords in arabic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_Ar = nltk.corpus.stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_Ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_Arـnopunct: إن يوماً باقياً من العمر  هو للمؤمن عمر ما ينبغى أن يستهان به\n",
      "text_Ar_tokens: ['إن', 'يوماً', 'باقياً', 'من', 'العمر', 'هو', 'للمؤمن', 'عمر', 'ما', 'ينبغى', 'أن', 'يستهان', 'به']\n",
      "text_Ar_nostop: ['يوماً', 'باقياً', 'العمر', 'للمؤمن', 'عمر', 'ينبغى', 'يستهان']\n"
     ]
    }
   ],
   "source": [
    "text_Ar='إن يوماً باقياً من العمر ... هو للمؤمن عمر ما ينبغى أن يستهان به'\n",
    "text_Arـnopunct = \"\".join([char for char in text_Ar if char not in string.punctuation])\n",
    "print('text_Arـnopunct:',text_Arـnopunct)\n",
    "text_Ar_tokens = word_tokenize(text_Arـnopunct)\n",
    "print('text_Ar_tokens:',text_Ar_tokens)\n",
    "text_Ar_nostop = [word for word in text_Ar_tokens if word not in stopwords_Ar]\n",
    "print('text_Ar_nostop:',text_Ar_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "If the concern is with the context (e.g. sentiment analysis) of the text it might make sense to treat words differently. For example, “Not” is included as stop word but when considering context of text, negation changes the so-called valence of a text. This needs to be treated carefully and is usually not trivial.\n",
    "\n",
    "Considering example for ‘not’-\n",
    "\n",
    "    NLTK is a useful tool \t\t=> NLTK useful tool \n",
    "    NLTK is NOT a useful tool \t=> NLTK useful tool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_1_nostop: ['NLTK', 'useful', 'tool']\n",
      "text_2_nostop: ['NLTK', 'useful', 'tool']\n"
     ]
    }
   ],
   "source": [
    "text_1='NLTK is a useful tool '\n",
    "text_2='NLTK is not a useful tool '\n",
    "text_1_nopunct = \"\".join([char for char in text_1 if char not in string.punctuation])\n",
    "text_2_nopunct = \"\".join([char for char in text_2 if char not in string.punctuation])\n",
    "text_1_tokens = word_tokenize(text_1_nopunct)\n",
    "text_2_tokens = word_tokenize(text_2_nopunct)\n",
    "\n",
    "text_1_nostop = [word for word in text_1_tokens if word not in stopwords_En]\n",
    "text_2_nostop = [word for word in text_2_tokens if word not in stopwords_En]\n",
    "print('text_1_nostop:',text_1_nostop)\n",
    "print('text_2_nostop:',text_2_nostop)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.5 Using Stemming\n",
    " \n",
    " \n",
    "    --Stemming.  Is the process of reducing inflected or derived words to their word stem or root. \n",
    "    --Stemming is aiming to reduce variations of the same root word. \n",
    "    \n",
    "    If Python sees play, playing, played and plays  as four different separate things, that means:\n",
    " it has to keep those Four separate words in memory. Imagine every variation of every root word. Maybe we have a thousand root words.\n",
    "The alternative in this play, playing, played and plays   example is applying the stemmer, all different words will become one word ; play, \n",
    " Python has to look at a lot more tokens without a stemmer and it doesn't know that these separate tokens are even related. \n",
    "In this case, we're not leaving it up to Python. We're being explicit by replacing similar words with just one common root word. \n",
    "reduce the corpus of words that exposed to the model\n",
    "Explicitly correlate the word with similar meaning \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are English and Non-English Stemmers available in nltk package. \n",
    "\n",
    "        --Porter Stemmer:  PorterStemmer is known for its simplicity and speed. PorterStemmer uses Suffix Stripping to produce stems\n",
    "        --Lancaster Stemmer:  LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur\n",
    "        --The Snowball Stemmer\n",
    "        --Regex-Based Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply_rule_list',\n",
       " '_contains_vowel',\n",
       " '_ends_cvc',\n",
       " '_ends_double_consonant',\n",
       " '_has_positive_measure',\n",
       " '_is_consonant',\n",
       " '_measure',\n",
       " '_replace_suffix',\n",
       " '_step1a',\n",
       " '_step1b',\n",
       " '_step1c',\n",
       " '_step2',\n",
       " '_step3',\n",
       " '_step4',\n",
       " '_step5a',\n",
       " '_step5b',\n",
       " 'mode',\n",
       " 'pool',\n",
       " 'stem',\n",
       " 'vowels']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('play'))\n",
    "print(ps.stem('playing'))\n",
    "print(ps.stem('played'))\n",
    "print(ps.stem('plays'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "univers\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "#Over-Steming \n",
    "#Over-steaming occurs when two words are stemmed from the same root of different stems. \n",
    "\n",
    "print(ps.stem('universal'))\n",
    "print(ps.stem('university'))\n",
    "print(ps.stem('universe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alumnu\n",
      "alumni\n",
      "alumna\n"
     ]
    }
   ],
   "source": [
    "#Under-Steming\n",
    "#Under-stemming occurs when two words are stemmed from the same root of not a different stems\n",
    "\n",
    "print(ps.stem('alumnus'))\n",
    "print(ps.stem('alumni'))\n",
    "print(ps.stem('alumnae'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunc</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</td>\n",
       "      <td>ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]</td>\n",
       "      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, bless, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even my brother is not like to speak with me they treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                                                              body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.   \n",
       "1                                           Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "2                                                                                                                                         Nah I don't think he goes to usf, he lives around here though   \n",
       "3                                                                                                                         Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                                                                                                                   I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                                                                                                                   body_text_nopunc  \\\n",
       "0  ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times   \n",
       "1                                             free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "2                                                                                                                                       nah i dont think he goes to usf he lives around here though   \n",
       "3                                                                                                                       even my brother is not like to speak with me they treat me like aids patent   \n",
       "4                                                                                                                                                                 i have a date on sunday with will   \n",
       "\n",
       "                                                                                                                                                                                       body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...   \n",
       "1                       [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "2                                                                                                                                [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3                                                                                                             [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                                                                                                                               [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                                                                                  body_text_nostop  \\\n",
       "0                                 [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "2                                                                                                             [nah, dont, think, goes, usf, lives, around, though]   \n",
       "3                                                                                                          [even, brother, like, speak, treat, like, aids, patent]   \n",
       "4                                                                                                                                                   [date, sunday]   \n",
       "\n",
       "                                                                                                                                             body_text_stemmed  \n",
       "0                                            [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, bless, time]  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]  \n",
       "2                                                                                                           [nah, dont, think, goe, usf, live, around, though]  \n",
       "3                                                                                                       [even, brother, like, speak, treat, like, aid, patent]  \n",
       "4                                                                                                                                               [date, sunday]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['body_text_stemmed'] = df['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.6 Using Lemmatizer\n",
    " \n",
    " Lemmatizing : The process of grouping together the inflected forms of a word so they can be analyzed as a single term.\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language\n",
    "lemmatizing is using vocabulary analysis of words to remove inflectional endings and return to the dictionary form of a word. \n",
    "So again : play, playing, played and plays  would all be simplified down to play, because that's the root of the word. Each variation carries the same meaning just with slightly different tense. \n",
    "Will use the WordNet lemmatizer. This is probably the most popular lemmatizer. \n",
    "WordNet is a collection of nouns, verbs, adjective and adverbs that are grouped together in sets of synonyms, each expressing a distinct concept. \n",
    "This lemmatizer runs off of this corpus of synonyms, so given a word, it will track that word to its synonyms, and then the distinct concept that that group of words represents. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ahmadshhadeh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'lemmatize']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universal\n",
      "university\n",
      "universe\n"
     ]
    }
   ],
   "source": [
    "print(wn.lemmatize('universal'))\n",
    "print(wn.lemmatize('university'))\n",
    "print(wn.lemmatize('universe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alumnus\n",
      "alumnus\n",
      "alumna\n"
     ]
    }
   ],
   "source": [
    "print(wn.lemmatize('alumnus'))\n",
    "print(wn.lemmatize('alumni'))\n",
    "print(wn.lemmatize('alumnae'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunc</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</td>\n",
       "      <td>ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]</td>\n",
       "      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, bless, time]</td>\n",
       "      <td>[ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even my brother is not like to speak with me they treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                                                              body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.   \n",
       "1                                           Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "2                                                                                                                                         Nah I don't think he goes to usf, he lives around here though   \n",
       "3                                                                                                                         Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                                                                                                                   I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                                                                                                                   body_text_nopunc  \\\n",
       "0  ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times   \n",
       "1                                             free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "2                                                                                                                                       nah i dont think he goes to usf he lives around here though   \n",
       "3                                                                                                                       even my brother is not like to speak with me they treat me like aids patent   \n",
       "4                                                                                                                                                                 i have a date on sunday with will   \n",
       "\n",
       "                                                                                                                                                                                       body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, i, wont, take, your, help, for, granted, and, will, fulfil, my, promise, you, have, been, wonderful,...   \n",
       "1                       [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "2                                                                                                                                [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3                                                                                                             [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                                                                                                                               [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                                                                                  body_text_nostop  \\\n",
       "0                                 [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "2                                                                                                             [nah, dont, think, goes, usf, lives, around, though]   \n",
       "3                                                                                                          [even, brother, like, speak, treat, like, aids, patent]   \n",
       "4                                                                                                                                                   [date, sunday]   \n",
       "\n",
       "                                                                                                                                             body_text_stemmed  \\\n",
       "0                                            [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, bless, time]   \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]   \n",
       "2                                                                                                           [nah, dont, think, goe, usf, live, around, though]   \n",
       "3                                                                                                       [even, brother, like, speak, treat, like, aid, patent]   \n",
       "4                                                                                                                                               [date, sunday]   \n",
       "\n",
       "                                                                                                                                              body_text_lemmatized  \n",
       "0                                   [ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, time]  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "2                                                                                                                [nah, dont, think, go, usf, life, around, though]  \n",
       "3                                                                                                           [even, brother, like, speak, treat, like, aid, patent]  \n",
       "4                                                                                                                                                   [date, sunday]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['body_text_lemmatized'] = df['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     #tokens = re.split('\\W+', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "df=df[['label','body_text']]\n",
    "df['cleaned_text'] = df['body_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5568, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</td>\n",
       "      <td>ive search right word thank breather promis wont take help grant fulfil promis wonder bless time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd txt ratetc appli 08452810075over18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                                                              body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.   \n",
       "1                                           Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "2                                                                                                                                         Nah I don't think he goes to usf, he lives around here though   \n",
       "3                                                                                                                         Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                                                                                                                   I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                                                          cleaned_text  \n",
       "0                                     ive search right word thank breather promis wont take help grant fulfil promis wonder bless time  \n",
       "1  free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd txt ratetc appli 08452810075over18  \n",
       "2                                                                                            nah dont think goe usf live around though  \n",
       "3                                                                                        even brother like speak treat like aid patent  \n",
       "4                                                                                                                          date sunday  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vectorizing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing :** The process that we use to convert text to a form that Python and a machine learning model can understand,will Creates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document.\n",
    "\n",
    "\n",
    "This is defined as the process of encoding text as integers to create feature vectors. \n",
    "A feature vector is an n-dimensional vector of numerical features that represent some object. So in our context, that means we'll be taking an individual text message and converting it to a numeric vector that represents that text message. \n",
    "\n",
    "\n",
    "There are many vectorization techniques, we will focus on the three widely used vectorization techniques:\n",
    "\n",
    "     --Count vectorization\n",
    "     --N-Grams.\n",
    "     --Term frequency - inverse document frequency (TF-IDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Vectorizing text data on sample \n",
    "\n",
    "\n",
    "We will create a matrix that only has numeric entries counting how many times each word appears in each text message. The machine learning algorithm understands these counts. So if it sees a one or a two or a three in a cell, then that model can start to correlate that with whatever we're trying to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"good movie\", \"not a good movie\", \"did not like\", \"i like it\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1: Count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n",
      "Sparse Matrix :\n",
      "   (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 2)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>good</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   did  good  it  like  movie  not\n",
       "0    0     1   0     0      1    0\n",
       "1    0     1   0     0      1    1\n",
       "2    1     0   0     1      0    1\n",
       "3    0     0   1     1      0    0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features_cv = vectorizer.fit_transform(sentences)\n",
    "print(features_cv.shape)\n",
    "print('Sparse Matrix :\\n', features_cv)\n",
    "features_cv = pd.DataFrame(features_cv.toarray())\n",
    "features_cv.columns = vectorizer.get_feature_names()\n",
    "features_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2: Vectorizing Data: N-Grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n",
      "Sparse Matrix :\n",
      "   (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>did not</th>\n",
       "      <th>did not like</th>\n",
       "      <th>good</th>\n",
       "      <th>good movie</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>like it</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>not good</th>\n",
       "      <th>not good movie</th>\n",
       "      <th>not like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   did  did not  did not like  good  good movie  it  like  like it  movie  \\\n",
       "0    0        0             0     1           1   0     0        0      1   \n",
       "1    0        0             0     1           1   0     0        0      1   \n",
       "2    1        1             1     0           0   0     1        0      0   \n",
       "3    0        0             0     0           0   1     1        1      0   \n",
       "\n",
       "   not  not good  not good movie  not like  \n",
       "0    0         0               0         0  \n",
       "1    1         1               1         0  \n",
       "2    1         0               0         1  \n",
       "3    0         0               0         0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ngram_vect = CountVectorizer(ngram_range=(1,3))\n",
    "features_ng = ngram_vect.fit_transform(sentences)\n",
    "print(features_ng.shape)\n",
    "print('Sparse Matrix :\\n', features_ng)\n",
    "features_ng = pd.DataFrame(features_ng.toarray())\n",
    "features_ng.columns = ngram_vect.get_feature_names()\n",
    "features_ng\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3: Vectorizing Data: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n",
      "Sparse Matrix :\n",
      "   (0, 4)\t0.5773502691896257\n",
      "  (0, 8)\t0.5773502691896257\n",
      "  (0, 3)\t0.5773502691896257\n",
      "  (1, 11)\t0.47212002654617047\n",
      "  (1, 10)\t0.47212002654617047\n",
      "  (1, 9)\t0.3722248517590162\n",
      "  (1, 4)\t0.3722248517590162\n",
      "  (1, 8)\t0.3722248517590162\n",
      "  (1, 3)\t0.3722248517590162\n",
      "  (2, 2)\t0.43671930987511215\n",
      "  (2, 12)\t0.43671930987511215\n",
      "  (2, 1)\t0.43671930987511215\n",
      "  (2, 6)\t0.3443145201184689\n",
      "  (2, 0)\t0.43671930987511215\n",
      "  (2, 9)\t0.3443145201184689\n",
      "  (3, 7)\t0.6176143709756019\n",
      "  (3, 5)\t0.6176143709756019\n",
      "  (3, 6)\t0.48693426407352264\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>did not</th>\n",
       "      <th>did not like</th>\n",
       "      <th>good</th>\n",
       "      <th>good movie</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>like it</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>not good</th>\n",
       "      <th>not good movie</th>\n",
       "      <th>not like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372225</td>\n",
       "      <td>0.372225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372225</td>\n",
       "      <td>0.372225</td>\n",
       "      <td>0.47212</td>\n",
       "      <td>0.47212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344315</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.436719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        did   did not  did not like      good  good movie        it      like  \\\n",
       "0  0.000000  0.000000      0.000000  0.577350    0.577350  0.000000  0.000000   \n",
       "1  0.000000  0.000000      0.000000  0.372225    0.372225  0.000000  0.000000   \n",
       "2  0.436719  0.436719      0.436719  0.000000    0.000000  0.000000  0.344315   \n",
       "3  0.000000  0.000000      0.000000  0.000000    0.000000  0.617614  0.486934   \n",
       "\n",
       "    like it     movie       not  not good  not good movie  not like  \n",
       "0  0.000000  0.577350  0.000000   0.00000         0.00000  0.000000  \n",
       "1  0.000000  0.372225  0.372225   0.47212         0.47212  0.000000  \n",
       "2  0.000000  0.000000  0.344315   0.00000         0.00000  0.436719  \n",
       "3  0.617614  0.000000  0.000000   0.00000         0.00000  0.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "#tfidf = TfidfVectorizer(min_df=1)\n",
    "tfidf = TfidfVectorizer( ngram_range=(1,3))\n",
    "features_tfidf = tfidf.fit_transform(sentences)\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n', features_tfidf)\n",
    "features_tfidf = pd.DataFrame(features_tfidf.toarray())\n",
    "features_tfidf.columns = tfidf.get_feature_names()\n",
    "features_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4  Vectorizers output sparse matrices\n",
    "\n",
    "_**Sparse Matrix**: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements._\n",
    "\n",
    "when you have a matrix in which a very high percent of the entries are zero, instead of storing all these zeros in the full matrix, which would make it extremely inefficient, it'll just be converted to only storing the locations and the values of the non-zero elements, which is much more efficient for storage![image.png]\n",
    "\n",
    "##\n",
    "\n",
    "    --features_tfidf = pd.DataFrame(features_tfidf.toarray())\n",
    "    \n",
    "    --features_tfidf.columns = tfidf.get_feature_names()\n",
    "    \n",
    "    --features_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Vectorizing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1: Count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 8090)\n",
      "Sparse Matrix :\n",
      "   (0, 3983)\t1\n",
      "  (0, 6219)\t1\n",
      "  (0, 6032)\t1\n",
      "  (0, 7835)\t1\n",
      "  (0, 7032)\t1\n",
      "  (0, 1681)\t1\n",
      "  (0, 5746)\t2\n",
      "  (0, 7826)\t1\n",
      "  (0, 6925)\t1\n",
      "  (0, 3571)\t1\n",
      "  (0, 3391)\t1\n",
      "  (0, 3187)\t1\n",
      "  (0, 7825)\t1\n",
      "  (0, 1568)\t1\n",
      "  (0, 7136)\t1\n",
      "  (1, 3127)\t1\n",
      "  (1, 2783)\t2\n",
      "  (1, 7805)\t1\n",
      "  (1, 2110)\t1\n",
      "  (1, 7771)\t1\n",
      "  (1, 2901)\t2\n",
      "  (1, 2279)\t1\n",
      "  (1, 3003)\t1\n",
      "  (1, 7158)\t1\n",
      "  (1, 452)\t1\n",
      "  :\t:\n",
      "  (5564, 3661)\t1\n",
      "  (5564, 3313)\t1\n",
      "  (5564, 3116)\t1\n",
      "  (5564, 2811)\t1\n",
      "  (5565, 6820)\t1\n",
      "  (5565, 4824)\t1\n",
      "  (5565, 5518)\t1\n",
      "  (5565, 6518)\t1\n",
      "  (5566, 3127)\t1\n",
      "  (5566, 4360)\t1\n",
      "  (5566, 7681)\t1\n",
      "  (5566, 5005)\t1\n",
      "  (5566, 7462)\t1\n",
      "  (5566, 6540)\t1\n",
      "  (5566, 1765)\t1\n",
      "  (5566, 2741)\t1\n",
      "  (5566, 3232)\t1\n",
      "  (5566, 3454)\t1\n",
      "  (5566, 3793)\t1\n",
      "  (5566, 3908)\t1\n",
      "  (5566, 986)\t1\n",
      "  (5566, 1552)\t1\n",
      "  (5567, 4927)\t1\n",
      "  (5567, 7296)\t1\n",
      "  (5567, 6060)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows × 8090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  0089mi  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0                0       0     0            0            0           0   0   \n",
       "1                0       0     0            0            0           0   0   \n",
       "2                0       0     0            0            0           0   0   \n",
       "3                0       0     0            0            0           0   0   \n",
       "4                0       0     0            0            0           0   0   \n",
       "...            ...     ...   ...          ...          ...         ...  ..   \n",
       "5563             0       0     0            0            0           0   0   \n",
       "5564             0       0     0            0            0           0   0   \n",
       "5565             0       0     0            0            0           0   0   \n",
       "5566             0       0     0            0            0           0   0   \n",
       "5567             0       0     0            0            0           0   0   \n",
       "\n",
       "      020603  0207  02070836089  ...  zero  zhong  zindgi  zoe  zogtoriu  \\\n",
       "0          0     0            0  ...     0      0       0    0         0   \n",
       "1          0     0            0  ...     0      0       0    0         0   \n",
       "2          0     0            0  ...     0      0       0    0         0   \n",
       "3          0     0            0  ...     0      0       0    0         0   \n",
       "4          0     0            0  ...     0      0       0    0         0   \n",
       "...      ...   ...          ...  ...   ...    ...     ...  ...       ...   \n",
       "5563       0     0            0  ...     0      0       0    0         0   \n",
       "5564       0     0            0  ...     0      0       0    0         0   \n",
       "5565       0     0            0  ...     0      0       0    0         0   \n",
       "5566       0     0            0  ...     0      0       0    0         0   \n",
       "5567       0     0            0  ...     0      0       0    0         0   \n",
       "\n",
       "      zoom  zouk  zyada  üll  〨ud  \n",
       "0        0     0      0    0    0  \n",
       "1        0     0      0    0    0  \n",
       "2        0     0      0    0    0  \n",
       "3        0     0      0    0    0  \n",
       "4        0     0      0    0    0  \n",
       "...    ...   ...    ...  ...  ...  \n",
       "5563     0     0      0    0    0  \n",
       "5564     0     0      0    0    0  \n",
       "5565     0     0      0    0    0  \n",
       "5566     0     0      0    0    0  \n",
       "5567     0     0      0    0    0  \n",
       "\n",
       "[5568 rows x 8090 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To create a Count Vectorizer, we simply need to instantiate one.\n",
    "# There are special parameters we can set here when making the vectorizer, but\n",
    "# for the most basic example, it is not needed.\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features_CountVec = vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(features_CountVec.shape)\n",
    "print('Sparse Matrix :\\n', features_CountVec)\n",
    "features_CountVec = pd.DataFrame(features_CountVec.toarray())\n",
    "features_CountVec.columns = vectorizer.get_feature_names()\n",
    "features_CountVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2: Vectorizing Data: N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 71114)\n",
      "Sparse Matrix :\n",
      "   (0, 32012)\t1\n",
      "  (0, 52679)\t1\n",
      "  (0, 51097)\t1\n",
      "  (0, 68874)\t1\n",
      "  (0, 60069)\t1\n",
      "  (0, 9211)\t1\n",
      "  (0, 48951)\t2\n",
      "  (0, 68761)\t1\n",
      "  (0, 58737)\t1\n",
      "  (0, 28173)\t1\n",
      "  (0, 26314)\t1\n",
      "  (0, 23047)\t1\n",
      "  (0, 68699)\t1\n",
      "  (0, 8482)\t1\n",
      "  (0, 61547)\t1\n",
      "  (0, 32090)\t1\n",
      "  (0, 52702)\t1\n",
      "  (0, 51188)\t1\n",
      "  (0, 68924)\t1\n",
      "  (0, 60080)\t1\n",
      "  (0, 9212)\t1\n",
      "  (0, 48973)\t1\n",
      "  (0, 68833)\t1\n",
      "  (0, 58823)\t1\n",
      "  (0, 28203)\t1\n",
      "  :\t:\n",
      "  (5566, 30234)\t1\n",
      "  (5566, 31689)\t1\n",
      "  (5566, 9814)\t1\n",
      "  (5566, 19004)\t1\n",
      "  (5566, 67204)\t1\n",
      "  (5566, 23403)\t1\n",
      "  (5566, 64906)\t1\n",
      "  (5566, 26815)\t1\n",
      "  (5566, 8404)\t1\n",
      "  (5566, 3613)\t1\n",
      "  (5566, 35441)\t1\n",
      "  (5566, 30235)\t1\n",
      "  (5566, 31690)\t1\n",
      "  (5566, 9815)\t1\n",
      "  (5566, 55820)\t1\n",
      "  (5566, 19005)\t1\n",
      "  (5566, 42869)\t1\n",
      "  (5566, 67205)\t1\n",
      "  (5566, 23404)\t1\n",
      "  (5567, 41870)\t1\n",
      "  (5567, 63280)\t1\n",
      "  (5567, 51387)\t1\n",
      "  (5567, 51390)\t1\n",
      "  (5567, 63304)\t1\n",
      "  (5567, 51391)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>008704050406 sp</th>\n",
       "      <th>008704050406 sp arrow</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0089mi last</th>\n",
       "      <th>0089mi last four</th>\n",
       "      <th>0121</th>\n",
       "      <th>0121 2025050</th>\n",
       "      <th>0121 2025050 visit</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>...</th>\n",
       "      <th>zyada kisi ko</th>\n",
       "      <th>üll</th>\n",
       "      <th>üll finish</th>\n",
       "      <th>üll finish buy</th>\n",
       "      <th>üll submit</th>\n",
       "      <th>üll submit da</th>\n",
       "      <th>üll take</th>\n",
       "      <th>üll take forev</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>〨ud even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows × 71114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  008704050406 sp  008704050406 sp arrow  0089mi  \\\n",
       "0                0                0                      0       0   \n",
       "1                0                0                      0       0   \n",
       "2                0                0                      0       0   \n",
       "3                0                0                      0       0   \n",
       "4                0                0                      0       0   \n",
       "...            ...              ...                    ...     ...   \n",
       "5563             0                0                      0       0   \n",
       "5564             0                0                      0       0   \n",
       "5565             0                0                      0       0   \n",
       "5566             0                0                      0       0   \n",
       "5567             0                0                      0       0   \n",
       "\n",
       "      0089mi last  0089mi last four  0121  0121 2025050  0121 2025050 visit  \\\n",
       "0               0                 0     0             0                   0   \n",
       "1               0                 0     0             0                   0   \n",
       "2               0                 0     0             0                   0   \n",
       "3               0                 0     0             0                   0   \n",
       "4               0                 0     0             0                   0   \n",
       "...           ...               ...   ...           ...                 ...   \n",
       "5563            0                 0     0             0                   0   \n",
       "5564            0                 0     0             0                   0   \n",
       "5565            0                 0     0             0                   0   \n",
       "5566            0                 0     0             0                   0   \n",
       "5567            0                 0     0             0                   0   \n",
       "\n",
       "      01223585236  ...  zyada kisi ko  üll  üll finish  üll finish buy  \\\n",
       "0               0  ...              0    0           0               0   \n",
       "1               0  ...              0    0           0               0   \n",
       "2               0  ...              0    0           0               0   \n",
       "3               0  ...              0    0           0               0   \n",
       "4               0  ...              0    0           0               0   \n",
       "...           ...  ...            ...  ...         ...             ...   \n",
       "5563            0  ...              0    0           0               0   \n",
       "5564            0  ...              0    0           0               0   \n",
       "5565            0  ...              0    0           0               0   \n",
       "5566            0  ...              0    0           0               0   \n",
       "5567            0  ...              0    0           0               0   \n",
       "\n",
       "      üll submit  üll submit da  üll take  üll take forev  〨ud  〨ud even  \n",
       "0              0              0         0               0    0         0  \n",
       "1              0              0         0               0    0         0  \n",
       "2              0              0         0               0    0         0  \n",
       "3              0              0         0               0    0         0  \n",
       "4              0              0         0               0    0         0  \n",
       "...          ...            ...       ...             ...  ...       ...  \n",
       "5563           0              0         0               0    0         0  \n",
       "5564           0              0         0               0    0         0  \n",
       "5565           0              0         0               0    0         0  \n",
       "5566           0              0         0               0    0         0  \n",
       "5567           0              0         0               0    0         0  \n",
       "\n",
       "[5568 rows x 71114 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ngram_vect = CountVectorizer(ngram_range=(1,3))\n",
    "features_ngram = ngram_vect.fit_transform(df['cleaned_text'])\n",
    "print(features_ngram.shape)\n",
    "print('Sparse Matrix :\\n', features_ngram)\n",
    "features_ngram = pd.DataFrame(features_ngram.toarray())\n",
    "features_ngram.columns = ngram_vect.get_feature_names()\n",
    "features_ngram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3: Vectorizing Raw Data: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 31293)\n",
      "Sparse Matrix :\n",
      "   (0, 3537)\t0.2581988897471611\n",
      "  (0, 30214)\t0.2581988897471611\n",
      "  (0, 21402)\t0.2581988897471611\n",
      "  (0, 10010)\t0.2581988897471611\n",
      "  (0, 11416)\t0.2581988897471611\n",
      "  (0, 12261)\t0.2581988897471611\n",
      "  (0, 25809)\t0.2581988897471611\n",
      "  (0, 30274)\t0.2581988897471611\n",
      "  (0, 21403)\t0.2581988897471611\n",
      "  (0, 3847)\t0.2581988897471611\n",
      "  (0, 26371)\t0.2581988897471611\n",
      "  (0, 30315)\t0.2581988897471611\n",
      "  (0, 22384)\t0.2581988897471611\n",
      "  (0, 23067)\t0.2581988897471611\n",
      "  (0, 13883)\t0.2581988897471611\n",
      "  (1, 2226)\t0.22133093562975756\n",
      "  (1, 21687)\t0.22133093562975756\n",
      "  (1, 28034)\t0.22133093562975756\n",
      "  (1, 21555)\t0.22133093562975756\n",
      "  (1, 8385)\t0.22133093562975756\n",
      "  (1, 21933)\t0.22133093562975756\n",
      "  (1, 1122)\t0.22133093562975756\n",
      "  (1, 8803)\t0.22133093562975756\n",
      "  (1, 26248)\t0.22133093562975756\n",
      "  (1, 554)\t0.22133093562975756\n",
      "  :\t:\n",
      "  (5563, 669)\t0.25369993188506684\n",
      "  (5563, 21270)\t0.2243853098438279\n",
      "  (5563, 20286)\t0.24928833692239552\n",
      "  (5563, 27763)\t0.20674035341619448\n",
      "  (5564, 9620)\t0.5773502691896257\n",
      "  (5564, 8434)\t0.5773502691896257\n",
      "  (5564, 10770)\t0.5773502691896257\n",
      "  (5565, 24378)\t0.5773502691896257\n",
      "  (5565, 17781)\t0.5773502691896257\n",
      "  (5565, 20608)\t0.5773502691896257\n",
      "  (5566, 28534)\t0.2829576397321421\n",
      "  (5566, 10168)\t0.2829576397321421\n",
      "  (5566, 29541)\t0.2829576397321421\n",
      "  (5566, 8187)\t0.2829576397321421\n",
      "  (5566, 4116)\t0.2829576397321421\n",
      "  (5566, 13720)\t0.2829576397321421\n",
      "  (5566, 13147)\t0.2829576397321421\n",
      "  (5566, 15359)\t0.2829576397321421\n",
      "  (5566, 1332)\t0.2829576397321421\n",
      "  (5566, 3497)\t0.2829576397321421\n",
      "  (5566, 11627)\t0.2829576397321421\n",
      "  (5566, 24471)\t0.26099893354680026\n",
      "  (5566, 18746)\t0.22619520767867973\n",
      "  (5567, 27850)\t0.7071067811865476\n",
      "  (5567, 22474)\t0.7071067811865476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406 sp</th>\n",
       "      <th>0089mi last</th>\n",
       "      <th>0121 2025050</th>\n",
       "      <th>01223585236 xx</th>\n",
       "      <th>01223585334 cum</th>\n",
       "      <th>0125698789 ring</th>\n",
       "      <th>02 user</th>\n",
       "      <th>020603 2nd</th>\n",
       "      <th>0207 153</th>\n",
       "      <th>02072069400 bx</th>\n",
       "      <th>...</th>\n",
       "      <th>zoe 18</th>\n",
       "      <th>zoe hit</th>\n",
       "      <th>zogtoriu stare</th>\n",
       "      <th>zoom cine</th>\n",
       "      <th>zouk nichol</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>üll finish</th>\n",
       "      <th>üll submit</th>\n",
       "      <th>üll take</th>\n",
       "      <th>〨ud even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows × 31293 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406 sp  0089mi last  0121 2025050  01223585236 xx  \\\n",
       "0                 0.0          0.0           0.0             0.0   \n",
       "1                 0.0          0.0           0.0             0.0   \n",
       "2                 0.0          0.0           0.0             0.0   \n",
       "3                 0.0          0.0           0.0             0.0   \n",
       "4                 0.0          0.0           0.0             0.0   \n",
       "...               ...          ...           ...             ...   \n",
       "5563              0.0          0.0           0.0             0.0   \n",
       "5564              0.0          0.0           0.0             0.0   \n",
       "5565              0.0          0.0           0.0             0.0   \n",
       "5566              0.0          0.0           0.0             0.0   \n",
       "5567              0.0          0.0           0.0             0.0   \n",
       "\n",
       "      01223585334 cum  0125698789 ring  02 user  020603 2nd  0207 153  \\\n",
       "0                 0.0              0.0      0.0         0.0       0.0   \n",
       "1                 0.0              0.0      0.0         0.0       0.0   \n",
       "2                 0.0              0.0      0.0         0.0       0.0   \n",
       "3                 0.0              0.0      0.0         0.0       0.0   \n",
       "4                 0.0              0.0      0.0         0.0       0.0   \n",
       "...               ...              ...      ...         ...       ...   \n",
       "5563              0.0              0.0      0.0         0.0       0.0   \n",
       "5564              0.0              0.0      0.0         0.0       0.0   \n",
       "5565              0.0              0.0      0.0         0.0       0.0   \n",
       "5566              0.0              0.0      0.0         0.0       0.0   \n",
       "5567              0.0              0.0      0.0         0.0       0.0   \n",
       "\n",
       "      02072069400 bx  ...  zoe 18  zoe hit  zogtoriu stare  zoom cine  \\\n",
       "0                0.0  ...     0.0      0.0             0.0        0.0   \n",
       "1                0.0  ...     0.0      0.0             0.0        0.0   \n",
       "2                0.0  ...     0.0      0.0             0.0        0.0   \n",
       "3                0.0  ...     0.0      0.0             0.0        0.0   \n",
       "4                0.0  ...     0.0      0.0             0.0        0.0   \n",
       "...              ...  ...     ...      ...             ...        ...   \n",
       "5563             0.0  ...     0.0      0.0             0.0        0.0   \n",
       "5564             0.0  ...     0.0      0.0             0.0        0.0   \n",
       "5565             0.0  ...     0.0      0.0             0.0        0.0   \n",
       "5566             0.0  ...     0.0      0.0             0.0        0.0   \n",
       "5567             0.0  ...     0.0      0.0             0.0        0.0   \n",
       "\n",
       "      zouk nichol  zyada kisi  üll finish  üll submit  üll take  〨ud even  \n",
       "0             0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "1             0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "2             0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "3             0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "4             0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "...           ...         ...         ...         ...       ...       ...  \n",
       "5563          0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "5564          0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "5565          0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "5566          0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "5567          0.0         0.0         0.0         0.0       0.0       0.0  \n",
       "\n",
       "[5568 rows x 31293 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "#tfidf = TfidfVectorizer(min_df=1)\n",
    "tfidf = TfidfVectorizer( ngram_range=(2,2))\n",
    "features_tfidf = tfidf.fit_transform(df['cleaned_text'])\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n', features_tfidf)\n",
    "features_tfidf = pd.DataFrame(features_tfidf.toarray())\n",
    "features_tfidf.columns = tfidf.get_feature_names()\n",
    "features_tfidf\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled15.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
